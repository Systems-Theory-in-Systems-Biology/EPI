import json
import os
from typing import Dict, Optional, Tuple, Union

import numpy as np

from eulerpi import logger

from .path_manager import PathManager


class ResultReader:
    """The result reader is responsible for loading the results of the inference from disk.

    Attributes:
        model_name(str): The name of the model (e.g. "temperature"). It is used to create the folder structure.
        run_name(str): The name of the run which shall be saved. It is used to create subfolders for different runs.
        inference_information(dict): A dictionary with information about the inference run, e.g. inference type, number of samples, etc.
        path_manager(PathManager): A path manager Object to manage file paths.
    """

    def __init__(
        self,
        model_name: Optional[str] = None,
        run_name: Optional[str] = None,
        path_manager: Optional[PathManager] = None,
    ) -> None:
        """Creates a result

        Args:
            model_name(str, optional): The name of the model (e.g. "temperature"). It is used to create the folder structure. Only required when no path manager is provided.
            run_name(str, optional): The name of the run which shall be saved. It is used to create subfolders for different runs. Only required when no path manager is provided.
            path_manager(PathManager, optional): A path manager Object to manage file paths. Defaiults to None creates a new path manager.
        """
        self.path_manager = path_manager or PathManager(model_name, run_name)
        self.model_name = self.path_manager.model_name
        self.run_name = self.path_manager.run_name
        self.inference_information = self.get_inference_information()

    def get_run_path(self) -> str:
        """Returns the path to the folder where the results for the given run are stored.

        Returns:
            str: The path to the folder where the results for the given run are stored.

        """
        return self.path_manager.get_run_path()

    def get_inference_information(self) -> Dict:
        """Load the inference information from the inference_information.json file.

        Returns:
            Dict: The inference information.
        """
        # return the Dict directly if already set
        if hasattr(self, "inference_information"):
            return self.inference_information

        inference_information_path = (
            self.path_manager.get_inference_information_path()
        )
        with open(inference_information_path, "r") as file:
            inference_information = json.load(file)

        return inference_information

    def load_inference_results(
        self,
        num_burn_in_samples: Optional[int] = None,
        thinning_factor: Optional[int] = None,
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Load the inference results generated by EPI.

        Args:
            num_burn_in_samples(int, optional): Number of samples that will be ignored per chain (i.e. walker). Only for mcmc inference. Default is None and uses the value that was used for the inference stored in inference_information.json.
            thinning_factor(int, optional): Thinning factor that will be used to thin the Markov chain. Only for mcmc inference. Default is None and uses the value that was used for the inference stored in each inference_information.json.

        Returns:
            typing.Tuple[np.ndarray, np.ndarray, np.ndarray]: The parameters, the pushforward of the parameters, and the density evaluations.

        """

        inference_information = self.get_inference_information()

        if inference_information["inference_type"] == "MCMC":
            return self.load_mcmc_inference_results(
                inference_information, num_burn_in_samples, thinning_factor
            )

        if num_burn_in_samples is not None or thinning_factor is not None:
            logger.info(
                f"For inference type {inference_information['inference_type']}, num_burn_in_samples and thinning_factor are ignored."
            )

        return self.load_grid_based_inference_results()

    def load_mcmc_inference_results(
        self,
        inference_information: Dict,
        num_burn_in_samples: Optional[int] = None,
        thinning_factor: Optional[int] = None,
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Load the inference results generated by EPI using MCMC.

        Args:
            num_burn_in_samples (int, optional): . Number of samples that will be ignored per chain (i.e. walker). Default is None and uses the value that was used for the inference stored in inference_information.json.
            thinning_factor(int, optional): Thinning factor that will be used to thin the Markov chain. Default is None and uses the value that was used for the inference stored in each inference_information.json.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: The parameters, the pushforward of the parameters, and the density evaluations.
        """
        run_path = self.get_run_path()

        # use default values saved in inference_information if not specified
        if num_burn_in_samples is None:
            num_burn_in_samples = inference_information["num_burn_in_samples"]

        if thinning_factor is None:
            thinning_factor = inference_information["thinning_factor"]

        num_steps = inference_information["num_steps"]
        num_walkers = inference_information["num_walkers"]

        # load samples from raw chains
        for i in range(inference_information["num_runs"]):
            params_current_chain = np.loadtxt(
                run_path + f"/Params/raw_params_{i}.csv",
                delimiter=",",
                ndmin=2,
            )
            pushforward_evals_current_chain = np.loadtxt(
                run_path + f"/PushforwardEvals/raw_pushforward_evals_{i}.csv",
                delimiter=",",
                ndmin=2,
            )
            density_evals_current_chain = np.loadtxt(
                run_path + f"/DensityEvals/raw_density_evals_{i}.csv",
                delimiter=",",
            )
            if i == 0:
                param_dim = params_current_chain.shape[1]
                data_dim = pushforward_evals_current_chain.shape[1]
                params = params_current_chain.reshape(
                    num_steps, num_walkers, param_dim
                )
                pushforward_evals = pushforward_evals_current_chain.reshape(
                    num_steps, num_walkers, data_dim
                )
                density_evals = density_evals_current_chain.reshape(
                    num_steps, num_walkers, 1
                )
            else:
                density_evals = np.concatenate(
                    (
                        density_evals,
                        density_evals_current_chain.reshape(
                            num_steps, num_walkers, 1
                        ),
                    )
                )
                pushforward_evals = np.concatenate(
                    (
                        pushforward_evals,
                        pushforward_evals_current_chain.reshape(
                            num_steps, num_walkers, data_dim
                        ),
                    )
                )
                params = np.concatenate(
                    (
                        params,
                        params_current_chain.reshape(
                            num_steps, num_walkers, param_dim
                        ),
                    )
                )

        # thin and burn in
        return (
            params[num_burn_in_samples::thinning_factor, :, :].reshape(
                -1, param_dim
            ),
            pushforward_evals[
                num_burn_in_samples::thinning_factor, :, :
            ].reshape(-1, data_dim),
            density_evals[num_burn_in_samples::thinning_factor, :, :].reshape(
                -1, 1
            ),
        )

    def load_grid_based_inference_results(
        self,
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Load the inference results generated by EPI using grid-based inference.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: The parameters, the pushforward of the parameters, and the density evaluations.
        """
        run_path = self.get_run_path()

        param_chain = np.loadtxt(
            run_path + "/params.csv",
            delimiter=",",
            ndmin=2,
        )
        pushforward_evals = np.loadtxt(
            run_path + "/pushforward_evals.csv",
            delimiter=",",
            ndmin=2,
        )
        density_evals = np.loadtxt(
            run_path + "/density_evals.csv",
            delimiter=",",
        )
        return (
            param_chain,
            pushforward_evals,
            density_evals,
        )

    def load_sampler_position(self) -> Union[np.ndarray, None]:
        """Load the current walker positions. If there are current walker positions defined by runs before this one, use them.

        Returns:
            Union[np.ndarray, None]: The current walker positions or None if there are no current walker positions.
        """
        # If there are current walker positions defined by runs before this one, use them.
        position_path = (
            self.get_run_path() + "/Params/current_walker_positions.csv"
        )
        if os.path.isfile(position_path):
            initial_walker_positions = np.loadtxt(
                position_path,
                delimiter=",",
                ndmin=2,
            )
            return initial_walker_positions
        return None
